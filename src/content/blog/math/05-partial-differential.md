---
title: "5. 偏微分と多変数関数への拡張"
description: "前回の微分を多変数関数へ拡張した偏微分を定義し、その幾何学的な意味を解説します。また、勾配ベクトルがなぜ偏微分から構成されるのかを詳述し、機械学習の最適化におけるその役割を明らかにします。"
date: 2025-07-25
tags: ["データサイエンスの数学", "微分積分"]
---

前回の記事では、勾配降下法が関数の「傾き」を利用して最適解を探すアルゴリズムであることを見ました。しかし、実際の機械学習における損失関数は、数千、数百万ものパラメータ（変数）を持つ多変数関数です。

このような多変数関数の「傾き」を捉えるために不可欠なのが、**偏微分 (partial differentiation)** の概念です。

***

### 偏微分とは？

偏微分とは、多変数関数のある**一つの変数にだけ注目し、他の変数を定数とみなして**微分する操作です。

例えば、$x$と$y$の2つの変数を持つ関数 $f(x, y) = x^2 + 3xy + y^2$ を考えます。

#### $x$についての偏微分
$f$を$x$について偏微分する場合、$y$をただの定数（例えば $y=c$）として扱います。偏微分は、通常の微分記号$d$の代わりに、$ \partial $（デル、パーシャル）という記号を使って表記します。

$$
\frac{\partial f}{\partial x} = \frac{\partial}{\partial x} (x^2 + 3xy + y^2)
$$

$y$を定数とみなして$x$で微分すると、
-   $x^2$ の微分は $2x$
-   $3xy$ （$3y$は定数）の微分は $3y$
-   $y^2$ （定数）の微分は $0$

よって、$x$に関する偏微分係数は以下のようになります。
$$
\frac{\partial f}{\partial x} = 2x + 3y
$$

#### $y$についての偏微分
同様に、$y$について偏微分する場合、$x$を定数とみなします。
$$
\frac{\partial f}{\partial y} = \frac{\partial}{\partial y} (x^2 + 3xy + y^2) = 0 + 3x + 2y = 3x + 2y
$$

***

### 幾何学的なイメージ

多変数関数 $z = f(x, y)$ は、3次元空間上に広がる「曲面」としてイメージできます。

-   $\frac{\partial f}{\partial x}$ は、その曲面を **$y$軸に平行な平面でスライスしたときの断面（曲線）の、x軸方向の傾き**を表します。
-   $\frac{\partial f}{\partial y}$ は、曲面を **$x$軸に平行な平面でスライスしたときの断面の、y軸方向の傾き**を表します。

つまり、偏微分は、高次元の複雑な地形において、特定の方角（軸方向）だけの傾斜を教えてくれるものと言えます。

***

### 勾配と偏微分の関係

前回の記事で紹介した**勾配 ($ \nabla f $)** の正体は、この**偏微分をすべての変数について計算し、それらを成分として並べたベクトル**です。

$n$個のパラメータ $ \theta_1, \dots, \theta_n $ を持つ損失関数 $L(\theta_1, \dots, \theta_n)$ の勾配は、以下のように定義されます。

$$
\nabla L = \left( \frac{\partial L}{\partial \theta_1}, \frac{\partial L}{\partial \theta_2}, \dots, \frac{\partial L}{\partial \theta_n} \right)
$$

勾配降下法の更新式 $ \theta_{\text{next}} = \theta_{\text{current}} - \eta \nabla L $ は、この勾配ベクトルを使って実行されます。具体的には、各パラメータ$ \theta_i $は、それぞれに対応する偏微分係数 $ \frac{\partial L}{\partial \theta_i} $ に従って、個別に更新されるのです。

-   $\frac{\partial L}{\partial \theta_i}$ が大きな正の値を持つ場合、パラメータ$ \theta_i $を少し減らすと、損失$L$が大きく減少することを示唆します。
-   $\frac{\partial L}{\partial \theta_i}$ が0に近い場合、そのパラメータ$ \theta_i $を変化させても、損失$L$はあまり変わらないことを示唆します。

このようにして、何百万ものパラメータを持つ巨大なモデルであっても、各パラメータが損失に与える影響を個別に計算し、同時に更新していくことで、効率的な学習が可能になります。この計算を高速に行うのが**誤差逆伝播法 (backpropagation)** であり、現代の深層学習を支える中核技術です。

***

### まとめ
-   **偏微分**は、多変数関数を「ある一つの変数にのみ着目して」微分する手法です。
-   各変数に関する偏微分をすべて集めてベクトルにしたものが**勾配**です。
-   勾配降下法は、この勾配（偏微分の集合）を利用して、多数のパラメータを同時に、かつ効率的に更新することで、損失関数の最小値を探します。